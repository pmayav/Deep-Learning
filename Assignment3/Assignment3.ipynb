{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVNj2nkTQGrN"
   },
   "source": [
    "# IDS 576: Assignment 3\n",
    "\n",
    "Patricia Maya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdl_ZRWAQGrP"
   },
   "source": [
    "# **1.** RNN for Language Modeling (4pt)\n",
    "\n",
    " -  Import the torchtext IMDB dataset and do the following:\n",
    "   -  Build a  Markov (n-gram) language model.\n",
    "   -  Change the output and the model appropriately in _[Simple Sentiment Analysis.ipynb](https://github.com/bentrevett/pytorch-sentiment-analysis)_ (also available [here](https://github.com/thejat/dl-notebooks/blob/master/examples/Seq2Seq_RNN_Simple_Sentiment_Analysis.ipynb) where the imports have been slightly modified) to build an LSTM based language model. Plot the training performance as a function of epochs/iterations.\n",
    " -  For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality.\n",
    " -  For each model, starting with the phrase \"My favorite movie \", sample the next few words and create an approx. 20 word generated review. Repeat this 5 times (you should ideally get different outputs each time) and report the outputs.\n",
    " - Note: make any assumptions as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxPe87idEywn"
   },
   "source": [
    "Language Modeling:\n",
    "- Give RNN a large text dataset\n",
    "- Model the probability of the next character given a sequence of previous characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYMH38uiNtu8"
   },
   "source": [
    "### Building a Markov (n-gram) language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pJfgB2SHIERy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bJe-d3DmjKE"
   },
   "source": [
    "#### Preparing Data\n",
    "One of the main concepts of TorchText is the `Field`. These define how your data should be processed.\n",
    "\n",
    "We use the `TEXT` field to define how the review should be processed.\n",
    "\n",
    "LABEL is defined by a LabelField, a special subset of the Field class specifically used for handling labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1rM8t3LwmLli"
   },
   "outputs": [],
   "source": [
    "N=4\n",
    "def make_ngrams(sequence):\n",
    "    \"\"\" Returns a list of n-long ngrams from a sequence \"\"\"\n",
    "    \"\"\" To change number of n just replace N above to desired n-gram  \"\"\"\n",
    "    n_grams = list(zip(*[sequence[i:] for i in range(N)]))\n",
    "    corpus = [] \n",
    "    for i in n_grams:\n",
    "        corpus.append(' '.join(i)) \n",
    "    return corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5-5z36iYmNSt"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', preprocessing=make_ngrams)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggS1NDpXmYRb",
    "outputId": "a2e55dda-d22c-4909-b547-f1b4bae75dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5O4V09_oeqy",
    "outputId": "4b44c413-f9c6-4984-a312-4688d7df5d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I love it when', 'love it when they', 'it when they actually', 'when they actually do', 'they actually do a', 'actually do a sports', 'do a sports story', 'a sports story well', 'sports story well .', 'story well . So', 'well . So many', '. So many in', 'So many in the', 'many in the past', 'in the past have', 'the past have been', 'past have been so', 'have been so hokey', 'been so hokey it', 'so hokey it was', 'hokey it was embarrassing', 'it was embarrassing to', 'was embarrassing to watch', 'embarrassing to watch .', 'to watch . Not', 'watch . Not this', '. Not this one', 'Not this one .', 'this one . It', \"one . It 's\", \". It 's just\", \"It 's just a\", \"'s just a genuinely\", 'just a genuinely nice', 'a genuinely nice movie', 'genuinely nice movie ,', 'nice movie , an', 'movie , an old', ', an old -', 'an old - fashioned', 'old - fashioned type', '- fashioned type of', 'fashioned type of story', 'type of story -', 'of story - and', 'story - and based', '- and based on', 'and based on a', 'based on a real', 'on a real -', 'a real - life', 'real - life guy', '- life guy to', 'life guy to did', 'guy to did exactly', 'to did exactly what', 'did exactly what Dennis', 'exactly what Dennis Quaid', 'what Dennis Quaid did', 'Dennis Quaid did in', 'Quaid did in this', 'did in this film', 'in this film .', 'this film . He', 'film . He plays', '. He plays a', 'He plays a high', 'plays a high school', 'a high school coach', 'high school coach who', 'school coach who is', 'coach who is talked', 'who is talked into', 'is talked into trying', 'talked into trying out', 'into trying out ,', 'trying out , late', 'out , late in', ', late in life', 'late in life athletically', 'in life athletically -', 'life athletically - speaking', 'athletically - speaking ,', '- speaking , to', 'speaking , to become', ', to become a', 'to become a pitcher', 'become a pitcher in', 'a pitcher in professional', 'pitcher in professional baseball', 'in professional baseball .', 'professional baseball . Eventually', 'baseball . Eventually ,', '. Eventually , he', 'Eventually , he reaches', ', he reaches his', 'he reaches his goal', 'reaches his goal of', 'his goal of making', 'goal of making it', 'of making it to', 'making it to the', 'it to the Major', 'to the Major Leagues', 'the Major Leagues ,', 'Major Leagues , even', 'Leagues , even if', ', even if it', 'even if it was', 'if it was a', 'it was a very', 'was a very brief', 'a very brief stint.<br', 'very brief stint.<br /><br', 'brief stint.<br /><br />All', 'stint.<br /><br />All the', '/><br />All the characters', '/>All the characters in', 'the characters in here', 'characters in here are', 'in here are nice', 'here are nice people', 'are nice people ,', 'nice people , the', 'people , the kind', ', the kind you', 'the kind you root', 'kind you root for', 'you root for ,', 'root for , from', 'for , from Quaid', ', from Quaid to', 'from Quaid to the', 'Quaid to the players', 'to the players on', 'the players on his', 'players on his high', 'on his high school', 'his high school team', 'high school team ,', 'school team , to', 'team , to his', ', to his little', 'to his little boy', 'his little boy (', 'little boy ( Angus', 'boy ( Angus T.', '( Angus T. Jones', 'Angus T. Jones ,', 'T. Jones , now', 'Jones , now somewhat', ', now somewhat of', 'now somewhat of a', 'somewhat of a star', 'of a star on', 'a star on television.)<br', 'star on television.)<br /><br', 'on television.)<br /><br />Quaid', 'television.)<br /><br />Quaid is', '/><br />Quaid is believable', '/>Quaid is believable in', 'is believable in playing', 'believable in playing Jim', 'in playing Jim Morris', 'playing Jim Morris because', 'Jim Morris because ,', 'Morris because , unlike', 'because , unlike actors', ', unlike actors in', 'unlike actors in the', 'actors in the past', 'in the past in', 'the past in sports', 'past in sports films', 'in sports films ,', 'sports films , he', 'films , he knows', ', he knows how', 'he knows how to', 'knows how to throw', 'how to throw a', 'to throw a baseball', 'throw a baseball .', 'a baseball . He', 'baseball . He looks', '. He looks like', 'He looks like a', 'looks like a pitcher', 'like a pitcher ,', 'a pitcher , a', 'pitcher , a guy', ', a guy who', 'a guy who could', 'guy who could fire', 'who could fire it', 'could fire it 90-plus', 'fire it 90-plus miles', 'it 90-plus miles per', '90-plus miles per hour', 'miles per hour .', 'per hour . And', 'hour . And ,', '. And , most', 'And , most of', ', most of this', 'most of this film', 'of this film is', 'this film is true', 'film is true ,', 'is true , as', 'true , as testified', ', as testified by', 'as testified by the', 'testified by the real', 'by the real -', 'the real - life', 'real - life pitcher', '- life pitcher in', 'life pitcher in one', 'pitcher in one the', 'in one the documentaries', 'one the documentaries on', 'the documentaries on the', 'documentaries on the DVD.<br', 'on the DVD.<br /><br', 'the DVD.<br /><br />So', 'DVD.<br /><br />So ,', '/><br />So , if', '/>So , if you', \", if you 're\", \"if you 're looking\", \"you 're looking for\", \"'re looking for a\", 'looking for a nice', 'for a nice ,', 'a nice , inspirational', 'nice , inspirational true', ', inspirational true life', 'inspirational true life sports', 'true life sports film', 'life sports film ,', 'sports film , you', 'film , you ca', \", you ca n't\", \"you ca n't wrong\", \"ca n't wrong with\", \"n't wrong with this\", 'wrong with this one', 'with this one .'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "#We can also check some examples.\n",
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbF-8lwJKtQR",
    "outputId": "6d876439-61ee-4b09-9ee4-70582389ed0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "#splitting train data into 70 train/30 validation\n",
    "import random\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "print(f'Number of training examples: {len(train_data)}') \n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUdVBEeTLCXn",
    "outputId": "3a228d11-9a69-4fef-b474-c2ed6b6c8875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n"
     ]
    }
   ],
   "source": [
    "#keeping ONLY the top 25,000 words. (most common max_size tokens.)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "#BUILD A VOCABULARY \n",
    "#This is a effectively a look up table where every unique word in your data set \n",
    "#has a corresponding _index_ (an integer).\n",
    "\n",
    "#Each index is used to construct a one-hot vector for each word. \n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "text_dictionary = dict(TEXT.vocab.freqs)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "#Why is the vocab size 25002 and not 25000? One of the addition tokens is the \n",
    "#`<unk>` token and the other is a `<pad>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wX1z6y8BbU0R",
    "outputId": "7a73bed7-77ce-4101-824e-37681917bec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('. < br /><br', 2826), ('! ! ! !', 1377), ('* * * *', 814), ('is one of the', 748), ('< br /><br />The', 649), ('the rest of the', 583), ('one of the most', 568), (\". I do n't\", 546), (\". It 's a\", 545), ('the end of the', 514)]\n"
     ]
    }
   ],
   "source": [
    "#see most common n-grams\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ymtzVACyclNj",
    "outputId": "977a33f4-1f92-4d34-da66-a42516b7cd49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_gram</th>\n",
       "      <th>frequency</th>\n",
       "      <th>n_minus1_gram</th>\n",
       "      <th>goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUST CAUSE showcases Sean</td>\n",
       "      <td>1</td>\n",
       "      <td>JUST CAUSE showcases</td>\n",
       "      <td>Sean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAUSE showcases Sean Connery</td>\n",
       "      <td>1</td>\n",
       "      <td>CAUSE showcases Sean</td>\n",
       "      <td>Connery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>showcases Sean Connery as</td>\n",
       "      <td>1</td>\n",
       "      <td>showcases Sean Connery</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sean Connery as a</td>\n",
       "      <td>1</td>\n",
       "      <td>Sean Connery as</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Connery as a Harvard</td>\n",
       "      <td>1</td>\n",
       "      <td>Connery as a</td>\n",
       "      <td>Harvard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n_gram  frequency           n_minus1_gram     goal\n",
       "0     JUST CAUSE showcases Sean          1    JUST CAUSE showcases     Sean\n",
       "1  CAUSE showcases Sean Connery          1    CAUSE showcases Sean  Connery\n",
       "2     showcases Sean Connery as          1  showcases Sean Connery       as\n",
       "3             Sean Connery as a          1         Sean Connery as        a\n",
       "4          Connery as a Harvard          1            Connery as a  Harvard"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a df with every n_gram and its frequency as well as n_gram - 1 and its respective target \n",
    "import pandas as pd\n",
    "x= N-1\n",
    "mkv_model = pd.DataFrame.from_dict(text_dictionary, orient=\"index\")\n",
    "mkv_model = mkv_model.reset_index()\n",
    "mkv_model.columns = [\"n_gram\",\"frequency\"]\n",
    "mkv_model[\"n_minus1_gram\"] = mkv_model.n_gram.apply(lambda i: \" \".join(i.split(\" \")[:x]))\n",
    "mkv_model[\"goal\"] = mkv_model.n_gram.apply(lambda i: i.split(\" \")[x])\n",
    "gram_count = pd.DataFrame(mkv_model.groupby(\"n_minus1_gram\",as_index=False)[\"frequency\"].sum())\n",
    "mkv_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fl-xIhf0qNw"
   },
   "source": [
    "### Build an LSTM based language model\n",
    "\n",
    "Change the output and the model appropriately in Simple Sentiment Analysis.ipynb (also available here where the imports have been slightly modified) to build an LSTM based language model. Plot the training performance as a function of epochs/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "U-FfvwW2OGlX",
    "outputId": "556b52f7-c279-4d7f-adb9-c9280a07e0b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz:   0%|          | 147k/84.1M [00:00<01:11, 1.18MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 60.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 5000\n",
      "Validation examples: 20000\n",
      "Testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "SEED=1234\n",
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED), split_ratio=0.20)\n",
    "\n",
    "print(f'Training examples: {len(train_data)}')\n",
    "print(f'Validation examples: {len(valid_data)}')\n",
    "print(f'Testing examples: {len(test_data)}')\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=2000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#creating the iterators. \n",
    "batch_size = 15\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = batch_size,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oH3wbiDKR2AB"
   },
   "outputs": [],
   "source": [
    "#building the model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, batch_size, output_dim):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim] - concatenation of the hidden state from every time step\n",
    "        #hidden = [1, batch size, hid dim] -  the final hidden state \n",
    "        \n",
    "        dim = output.size()\n",
    "        output = output.view(-1, output.shape[2])\n",
    "        new_output = F.log_softmax(output, dim=1)\n",
    "        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "\n",
    "        if batch_size==dim[1]:\n",
    "            new_output = new_output.view(-1, OUTPUT_DIM, batch_size)\n",
    "        else:\n",
    "            new_output = new_output.view(dim[1], OUTPUT_DIM,-1)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]       \n",
    "        return new_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EkKY0daf6Evu"
   },
   "outputs": [],
   "source": [
    "#create an instance of our RNN class\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab) # the dimension of the one-hot vectors, which is equal to the vocabulary size.\n",
    "EMBEDDING_DIM = 100 #size of the dense word vectors, usually around 50-250 dimensions\n",
    "OUTPUT_DIM = len(TEXT.vocab) \n",
    "\n",
    "model = LSTM(INPUT_DIM, EMBEDDING_DIM, batch_size,OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Lr5P291pS9N-",
    "outputId": "1923897d-518a-4e2b-bd54-e3624e316319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17,049,032 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#function that will tell us how many trainable parameters our model has so we can compare \n",
    "#the number of parameters across different models.\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKh9ycb6TFOf"
   },
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dfc_KFBSTEyn"
   },
   "outputs": [],
   "source": [
    "#create an optimizer using stochastic gradient descent (SGD)\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "#first argument: parameters will be updated by the optimizer, the second: learning rate\n",
    "\n",
    "#define our loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#Using .to, we can place the model and the criterion on the GPU (if we have one).\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HLTNXH7SL7xK"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion,BATCH_SIZE):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_label_count = 0 \n",
    "    loss=0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()   \n",
    "        predictions = model(batch.text)\n",
    "\n",
    "        dim = predictions.size()\n",
    "        if dim[2] !=BATCH_SIZE:\n",
    "            BATCH_SIZE = dim[2]\n",
    "            \n",
    "        pad = torch.tensor([1]*BATCH_SIZE,device=device).view(BATCH_SIZE,-1)\n",
    "        _,preds = torch.max(predictions,1)\n",
    "        labels = batch.text.view(-1,BATCH_SIZE)\n",
    "        labels = labels[1:]\n",
    "        pad = torch.tensor([1]*BATCH_SIZE,device=device).view(-1,BATCH_SIZE)\n",
    "        labels = torch.cat((labels,pad),0)\n",
    "        loss = criterion(predictions,labels)\n",
    "        acc = torch.sum(preds == labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_label_count+= labels.numel()\n",
    "        \n",
    "    return epoch_loss / len(iterator) , (epoch_acc /epoch_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "xbf8KyCyOB31",
    "outputId": "1a33b7bd-3f4d-4806-c09e-958c8a982de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t Train Loss: 7.513 | Train Acc: 3.35%\n",
      "Epoch: 02\n",
      "\t Train Loss: 7.411 | Train Acc: 4.31%\n",
      "Epoch: 03\n",
      "\t Train Loss: 7.340 | Train Acc: 4.01%\n",
      "Epoch: 04\n",
      "\t Train Loss: 7.273 | Train Acc: 4.35%\n",
      "Epoch: 05\n",
      "\t Train Loss: 7.247 | Train Acc: 4.57%\n"
     ]
    }
   ],
   "source": [
    "#train the model through multiple epochs\n",
    "N_EPOCHS = 5\n",
    "training_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, batch_size)\n",
    "    training_losses.append(train_loss)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U1P74OVKSjE"
   },
   "source": [
    "Training performance as a function of epochs/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "LVrmcTAHKSFF",
    "outputId": "e3ff777d-3fe3-4f23-e5ee-4bb13e540b21"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=range(1, N_EPOCHS +1) , y=training_losses, c='red')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHe9rA9JM-Wi"
   },
   "source": [
    "- For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality.\n",
    "\n",
    "\n",
    "In order to build the Markov (n-gram) language model, I decided to use n=4 (quadrigram) as I saw an example that trained with Shakespeare as corpus with n=4 and the predictions by the model were very good. \n",
    "\n",
    "\n",
    "To build an LSTM based language model, I added the input layer, then the embediding layer, and lastly the lstm layer. \n",
    "\n",
    "It is important to note that I only trained the LSTM model with 5000 training examples and only take the top 2000 most common words and a small epoch number because I did not have enough RAM. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26o8F3b7Mfq5"
   },
   "source": [
    " -  For each model, starting with the phrase \"My favorite movie \", sample the next few words and create an approx. 20 word generated review. Repeat this 5 times (you should ideally get different outputs each time) and report the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QR0KpSwoT5w"
   },
   "source": [
    "Generating reviews for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ywipX_a7Yqwx",
    "outputId": "811341a0-3c73-4a74-d4db-00261eb66600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New generated review 1:\n",
      "\t  My favorite movie your seemed time anyone In American actually short performances early used me. New starts later.\n",
      "New generated review 2:\n",
      "\t  My favorite movie TV New times care i small reason of after it's she The early what movie..\n",
      "New generated review 3:\n",
      "\t  My favorite movie camera remember low on comedy looks fact my care favorite His <unk> been try lost.\n",
      "New generated review 4:\n",
      "\t  My favorite movie she's the them. film, small just action acting shows favorite our had early scene go.\n",
      "New generated review 5:\n",
      "\t  My favorite movie couldn't done nothing  head only guy fun instead recommend job isn't understand final movie..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for j in range(5):\n",
    "    phrase = torch.tensor([[TEXT.vocab.stoi[\"My\"]],[TEXT.vocab.stoi[\"favorite\"]],[TEXT.vocab.stoi[\"movie\"]]],device=\"cuda:0\")\n",
    "    for i in range(15):\n",
    "        x = model(phrase) #size [1, 2002, 3]\n",
    "        x = x.squeeze(0) #size [2002, 3]\n",
    "        x = x[: , x.size()[1]-1].detach().cpu().numpy()\n",
    "        x = np.sort(x)[::-1]\n",
    "        x = x[:500] \n",
    "        new_phrase = torch.tensor(np.where(op==np.random.choice(op,1)),device=\"cuda:0\")\n",
    "        phrase = torch.cat((phrase , new_phrase))  \n",
    "    newly_generated = \"\"\n",
    "    for y in phrase:\n",
    "        newly_generated = newly_generated + \" \" + TEXT.vocab.itos[y]\n",
    "    gen_length = len(newly_generated.split(sep=\" \"))\n",
    "    print(f'New generated review {str(j+1)}:')\n",
    "    print(f'\\t {newly_generated}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md8WonZ7QGrV"
   },
   "source": [
    "# **2.** Sequence to Sequence Model for Translation (4pt)\n",
    "\n",
    " -  Train the sequence to sequence model ([Model 1](https://github.com/thejat/dl-notebooks/blob/master/examples/Seq2Seq_Translation_Example.ipynb)) for a language pair (excluding French-English), where the output is English and the input is a language of [your choice](https://www.manythings.org/anki/).\n",
    " -  Now train another model (Model 2) for the reverse (i.e., from English to the language you chose). In this model, use the GloVe 100 dimensional embeddings. See notebook 4, cell 2 for an [example](https://github.com/bentrevett/pytorch-sentiment-analysis) while training.\n",
    " -  Input 5 well formed sentences from the English vocab to Model 2, and input the resultant translated sentences to Model 1. Display all model outputs in each case.\n",
    " - Note: make any assumptions as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrQ5xTFGiOqE"
   },
   "source": [
    "In this project we will be teaching a neural network to translate from **Spanish to English**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code of part 2 adapted from various sources: \n",
    "\n",
    "https://gist.github.com/martinpella/a22bb1d4687f1ab9eb6679e0c10729ad#file-glove_load-py\n",
    "\n",
    "https://github.com/thejat/dl-notebooks/blob/master/examples/Seq2Seq_Translation_Example.ipynb\n",
    "\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "\n",
    "https://gist.github.com/martinpella/79437e77fe77dfacb936bfbbea1a216f#file-weights_matrix-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "n5fhobJelgZZ",
    "outputId": "9ff18bfd-d514-44db-99d3-98314a39e985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-szWpLYJLR64",
    "outputId": "ac4f9448-b0a3-4e62-e708-576d85576b6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_UbxOHvmkA6R"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<SOS>\": 0,\"<EOS>\": 1}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DppP-7ZykB4s"
   },
   "outputs": [],
   "source": [
    "#The files are all in Unicode, to simplify we will turn Unicode characters \n",
    "#to ASCII, make everything lowercase, and trim most punctuation.\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjcEbndkkqRZ"
   },
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English → Other Language, so if we want to translate from Other Language → English I added the reverse flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k9PMnI3TkB_7"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('/content/drive/MyDrive/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqCk7s1Qk528"
   },
   "source": [
    "Since there are a lot of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XEtGcQTTkCDE"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \")\n",
    "\n",
    "def filterPair(p, reverse = False):\n",
    "    if reverse == False:\n",
    "        return len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "                len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "                p[0].startswith(eng_prefixes)           \n",
    "    else:\n",
    "      return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "                len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "                p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs, reverse=False):\n",
    "    return [pair for pair in pairs if filterPair(pair, reverse=reverse)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wrAHW9ilERZ"
   },
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "- Read text file and split into lines, split lines into pairs\n",
    "- Normalize text, filter by length and content\n",
    "- Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wreRLhPqkCGf"
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs, reverse=reverse)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNf1NDoWuHs9",
    "outputId": "7c9c1e90-6171-4118-e9d1-c21e9eefe996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['el accidente es culpa suya .', 'he is to blame for the accident .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'spa', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR-vaCgELGuS"
   },
   "source": [
    "## Sequence to Sequence network\n",
    "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the black cat\". Most of the words in the input sentence have a direct translation in the output sentence, but are in slightly different orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\" construction there is also one more word in the input sentence. It would be difficult to produce a correct translation directly from the sequence of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the \"meaning\" of the input sequence into a single vector — a single point in some N dimensional space of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEKN71geL4t4"
   },
   "source": [
    "### The Encoder\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H2KLX_YBz59_"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36QnwqbDMQrL"
   },
   "source": [
    "### Attention Decoder\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wexcUvV7z6A2"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak0VKyueORgc"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CWjPPywOm3_"
   },
   "source": [
    "### Preparing Training Data\n",
    "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xRD3OzXCz6FR"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq99k2BZOfGo"
   },
   "source": [
    "#### Training fucntions\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w1mTxbX4z6Ia"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "####\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn8vkM3JO91m"
   },
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "- Start a timer\n",
    "- Initialize optimizers and criterion\n",
    "- Create set of training pairs\n",
    "- Start empty losses array for plotting\n",
    "- Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Pxw7GtGIPIlM"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "#plotting results \n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIJEOmNfPzlk"
   },
   "source": [
    "#### Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W3eHw51xPfzu"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "#We can evaluate random sentences from the training set and print out the input,\n",
    "#target, and output to make some subjective quality judgements\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1sgFlPdQd5N"
   },
   "source": [
    "## Training & Evaluating Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcd2dMisQdU5",
    "outputId": "d698ad20-e5d0-4799-a2f5-04862ed52ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 36s (- 78m 34s) (5000 6%) 3.0159\n",
      "11m 15s (- 73m 12s) (10000 13%) 2.3936\n",
      "16m 52s (- 67m 31s) (15000 20%) 1.9997\n",
      "22m 30s (- 61m 52s) (20000 26%) 1.7157\n",
      "28m 9s (- 56m 18s) (25000 33%) 1.4706\n",
      "33m 45s (- 50m 37s) (30000 40%) 1.2758\n",
      "39m 27s (- 45m 5s) (35000 46%) 1.1324\n",
      "45m 5s (- 39m 27s) (40000 53%) 0.9368\n",
      "50m 46s (- 33m 50s) (45000 60%) 0.8584\n",
      "56m 24s (- 28m 12s) (50000 66%) 0.7409\n",
      "62m 0s (- 22m 32s) (55000 73%) 0.6590\n",
      "67m 37s (- 16m 54s) (60000 80%) 0.5891\n",
      "73m 15s (- 11m 16s) (65000 86%) 0.5108\n",
      "78m 55s (- 5m 38s) (70000 93%) 0.4644\n",
      "84m 33s (- 0m 0s) (75000 100%) 0.4162\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc9GIvlErPgg",
    "outputId": "cd05e145-8c2e-4642-93e8-936981bd03c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> sos importante .\n",
      "= you are important .\n",
      "< you are important . <EOS>\n",
      "\n",
      "> mola tocar m sica contigo .\n",
      "= you re fun to play music with .\n",
      "< you re fun to play music with . <EOS>\n",
      "\n",
      "> ella ama las flores .\n",
      "= she is very fond of flowers .\n",
      "< she is very fond of flowers . <EOS>\n",
      "\n",
      "> lo estoy esperando .\n",
      "= i m waiting for him .\n",
      "< i m waiting for you . <EOS>\n",
      "\n",
      "> estoy cansado de la tele .\n",
      "= i m tired of tv .\n",
      "< i am tired of tv . <EOS>\n",
      "\n",
      "> no tienes edad para tomar .\n",
      "= you re not old enough to drink .\n",
      "< you re not old enough to . . <EOS>\n",
      "\n",
      "> est s empapado .\n",
      "= you re soaking wet .\n",
      "< you re soaking wet . <EOS>\n",
      "\n",
      "> soy un artista .\n",
      "= i am an artist .\n",
      "< i am an artist . <EOS>\n",
      "\n",
      "> ellos est n bien .\n",
      "= they re fine .\n",
      "< they re fine . <EOS>\n",
      "\n",
      "> estamos todos invitados .\n",
      "= we re all invited .\n",
      "< we re all invited . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLTDWPNurlej"
   },
   "source": [
    "- Now train another model (Model 2) for the reverse (i.e., from English to the language you chose). In this model, use the GloVe 100 dimensional embeddings. See notebook 4, cell 2 for an example while training.\n",
    "\n",
    "Pre-trained word vectors Glove.6B -- Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd3AZ1SeQEj5"
   },
   "source": [
    "#### Load GloVe 100\n",
    "\n",
    "Given that the vocabulary has 400k tokens, will use bcolz to store the array of vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUBi8qUxHgr5",
    "outputId": "ccb67e37-a608-491b-b051-5cf869c6a936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zgMdsy9IPbFH"
   },
   "outputs": [],
   "source": [
    "#!pip install bcolz\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "\n",
    "path = '/content/drive/MyDrive'\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'{path}/6B.100.dat', mode='w')\n",
    "\n",
    "with open(f'{path}/glove.6B.100d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "\n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 100)), rootdir=f'{path}/6B.100.dat', mode='w')\n",
    "vectors.flush()\n",
    "\n",
    "pickle.dump(words, open(f'{path}/6B.100_words.pkl' , 'wb'))\n",
    "pickle.dump(word2idx, open(f'{path}/6B.100_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GBortBlTRPgq"
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "\n",
    "vectors = bcolz.open(f'{path}/6B.100.dat')[:]\n",
    "words = pickle.load(open(f'{path}/6B.100_words.pkl' , 'rb'))\n",
    "word2idx = pickle.load(open(f'{path}/6B.100_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "r0Wmsd_0SfIa",
    "outputId": "1356f85f-7d2c-4701-9abf-748f2d2cc573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the vector for the word 'the'\n",
    "glove['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIcJBmYGUuLV"
   },
   "source": [
    "Now, we create an embedding layer that is a dictionary mapping integer indices to dense vectors. \n",
    "\n",
    "For each word in dataset’s vocabulary, we check if it is on GloVe’s vocabulary. If it is, we load its pre-trained word vector. Otherwise, we initialize a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "3raEFB2vbKMe",
    "outputId": "34f13227-56cf-4c6a-ba28-b591fe94b690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 128084 sentence pairs\n",
      "Trimmed to 7504 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2623\n",
      "spa 3775\n",
      "['he is a biologist .', ' l es bi logo .']\n",
      "The word \"<SOS>\" is not in GloVe 100\n",
      "The word \"<EOS>\" is not in GloVe 100\n",
      "The word \"housesitting\" is not in GloVe 100\n",
      "The word \"overemotional\" is not in GloVe 100\n"
     ]
    }
   ],
   "source": [
    "#load & prepare pairs of words : english to spanish\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "vocab_filter = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \")\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'spa', False)\n",
    "print(random.choice(pairs))\n",
    "\n",
    "#For each word in dataset’s vocabulary, we check if it is on GloVe’s vocabulary.\n",
    "#If yes - load its pre-trained word vector. \n",
    "target_vocab = input_lang.word2index\n",
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 100))\n",
    "words_found = 0\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        print(f'The word \"{word}\" is not in GloVe 100')\n",
    "\n",
    "weights_matrix = torch.from_numpy(weights_matrix).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayFFtoWnirxF"
   },
   "source": [
    "We now create a neural network with an embedding layer as first layer (we load into it the weights matrix) and a GRU layer. When doing a forward pass we must call first the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5YWVBjb7hkwK"
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape \n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ToyNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, batch_size=1):\n",
    "        super(ToyNN, self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        embedded = self.embedding(inp).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6T_KfBxqjrH"
   },
   "source": [
    "#### Train Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_ad6DJfwlg0r"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder2 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "VYChz-i4rCJ6",
    "outputId": "428405d0-d85e-4d4a-deb3-a58782170d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6m 32s (- 91m 40s) (5000 6%) 3.9017\n",
      "13m 0s (- 84m 33s) (10000 13%) 3.1469\n",
      "19m 33s (- 78m 14s) (15000 20%) 2.7233\n",
      "26m 45s (- 73m 34s) (20000 26%) 2.4051\n",
      "35m 8s (- 70m 17s) (25000 33%) 2.1470\n",
      "43m 21s (- 65m 1s) (30000 40%) 1.8949\n",
      "51m 24s (- 58m 45s) (35000 46%) 1.7087\n",
      "59m 26s (- 52m 0s) (40000 53%) 1.5404\n",
      "67m 39s (- 45m 6s) (45000 60%) 1.4169\n",
      "75m 51s (- 37m 55s) (50000 66%) 1.2881\n",
      "84m 3s (- 30m 34s) (55000 73%) 1.1854\n",
      "92m 19s (- 23m 4s) (60000 80%) 1.0682\n",
      "100m 35s (- 15m 28s) (65000 86%) 0.9817\n",
      "108m 50s (- 7m 46s) (70000 93%) 0.9491\n",
      "117m 3s (- 0m 0s) (75000 100%) 0.8900\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder2, attn_decoder2, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYfZlVpvrUEF"
   },
   "source": [
    "### Evaluate Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "p682THoZrGZn",
    "outputId": "a3baa168-6c60-4370-dd85-6b6549e1b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> they are generous with their money .\n",
      "= son generosos con su dinero .\n",
      "< son generosos con su dinero . <EOS>\n",
      "\n",
      "> she s much heavier than him .\n",
      "= ella es mucho m s pesada que l .\n",
      "< ella es mucho m s que l . <EOS>\n",
      "\n",
      "> i m from australia .\n",
      "= soy de australia .\n",
      "< soy de australia . <EOS>\n",
      "\n",
      "> i m looking for a travel agency .\n",
      "= estoy buscando una agencia de viajes .\n",
      "< estoy buscando una agencia de viajes . <EOS>\n",
      "\n",
      "> i m not convinced .\n",
      "= no estoy convencida .\n",
      "< no estoy convencida . <EOS>\n",
      "\n",
      "> i m afraid to go into the cave .\n",
      "= me da miedo entrar en la cueva .\n",
      "< me da miedo de la prueba . <EOS>\n",
      "\n",
      "> you re beautiful .\n",
      "= sos hermosa .\n",
      "< sos hermosa . <EOS>\n",
      "\n",
      "> i m not going to take that risk .\n",
      "= no voy a correr ese riesgo .\n",
      "< no voy no voy a correr . <EOS>\n",
      "\n",
      "> we re not the bad guys .\n",
      "= no somos los malos de la pel cula .\n",
      "< no somos los malos de la pel cula . <EOS>\n",
      "\n",
      "> i m pleased to meet you .\n",
      "= un placer conocerte .\n",
      "< encantado de conocerle . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, attn_decoder2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
